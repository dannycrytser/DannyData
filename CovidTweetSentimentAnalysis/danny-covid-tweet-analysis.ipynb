{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('covid19_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166656\n",
      "                               user_name  \\\n",
      "0                                 ·èâ·é•‚òª’¨ÍÇÖœÆ   \n",
      "1                          Tom Basile üá∫üá∏   \n",
      "2                        Time4fisticuffs   \n",
      "3                            ethel mertz   \n",
      "4                               DIPR-J&K   \n",
      "5                       üéπ Franz Schubert   \n",
      "6                           hr bartender   \n",
      "7                         Derbyshire LPC   \n",
      "8                      Prathamesh Bendre   \n",
      "9      Member of Christ üá®üá≥üá∫üá∏üáÆüá≥üáÆüá©üáßüá∑üá≥üá¨üáßüá©üá∑üá∫   \n",
      "10                Voice Of CBSE Students   \n",
      "11                           Creativegms   \n",
      "12                            SEXXYLYPPS   \n",
      "13    Africa Youth Advisory Board on DRR   \n",
      "14                        DailyaddaaNews   \n",
      "15                         Dimapur 24/7.   \n",
      "16                        ChennaiCityNow   \n",
      "17                  marc goovaertsüá™üá∫üè≥Ô∏è‚Äçüåà   \n",
      "18                            Dorian Aur   \n",
      "19                       Coronavirus Law   \n",
      "20                     The Voice of GenX   \n",
      "21                     APO Group English   \n",
      "22                          Micah Pollak   \n",
      "23                                 CAWST   \n",
      "24                        Florian Bieber   \n",
      "25                    Blood Donors India   \n",
      "26  Tetra Tech International Development   \n",
      "27            beatnikgeek the soothsayer   \n",
      "28                  Mugilan Chandrakumar   \n",
      "29               üèéüöíüö®‚òéÔ∏è‚öñÔ∏è‚òéÔ∏èüì©üì•üìß TAX Reform   \n",
      "\n",
      "                                        user_location  \\\n",
      "0                                          astroworld   \n",
      "1                                        New York, NY   \n",
      "2                                    Pewee Valley, KY   \n",
      "3                                Stuck in the Middle    \n",
      "4                                   Jammu and Kashmir   \n",
      "5                                         –ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è   \n",
      "6                                     Gainesville, FL   \n",
      "7                                                 NaN   \n",
      "8                                                 NaN   \n",
      "9                          üëáüèªlocation at link belowüëáüèª   \n",
      "10                                                NaN   \n",
      "11                                   Dhaka,Bangladesh   \n",
      "12  Hotel living - various cities!  Who needs a ho...   \n",
      "13                                             Africa   \n",
      "14                                          New Delhi   \n",
      "15                                    Nagaland, India   \n",
      "16                                                NaN   \n",
      "17                                           Brussels   \n",
      "18                                                NaN   \n",
      "19                                       Florida, USA   \n",
      "20                                                NaN   \n",
      "21                                      #AFRICA #MENA   \n",
      "22                                  Northwest Indiana   \n",
      "23                                     100+ countries   \n",
      "24                                               Graz   \n",
      "25                                      Mumbai, India   \n",
      "26                          Working in 175+ countries   \n",
      "27                                      Manhattan, NY   \n",
      "28                      no e-pass to cross borders...   \n",
      "29                                   I ‚ô•Ô∏è I ‚ô•Ô∏è I ‚ô•Ô∏è I   \n",
      "\n",
      "                                     user_description         user_created  \\\n",
      "0   wednesday addams as a disney princess keepin i...  2017-05-26 05:46:42   \n",
      "1   Husband, Father, Columnist & Commentator. Auth...  2009-04-16 20:06:23   \n",
      "2   #Christian #Catholic #Conservative #Reagan #Re...  2009-02-28 18:57:41   \n",
      "3   #Browns #Indians #ClevelandProud #[]_[] #Cavs ...  2019-03-07 01:45:06   \n",
      "4   üñäÔ∏èOfficial Twitter handle of Department of Inf...  2017-02-12 06:45:15   \n",
      "5   üéº  #–ù–æ–≤–æ—Ä–æÃÅ—Å—Å–∏—è #Novorossiya #–æ—Å—Ç–∞–≤–∞–π—Å—è–¥–æ–º–∞ #S...  2018-03-19 16:29:52   \n",
      "6   Workplace tips and advice served up in a frien...  2008-08-12 18:19:49   \n",
      "7                                                 NaN  2012-02-03 18:08:10   \n",
      "8    A poet, reiki practitioner and a student of law.  2015-04-25 08:15:41   \n",
      "9   Just as the body is one & has many members, & ...  2014-08-17 04:53:22   \n",
      "10                                                NaN  2020-07-14 17:50:30   \n",
      "11  I'm Motalib Mia, Logo -Logo Designer - Brandin...  2020-01-12 09:03:01   \n",
      "12  My ink \"My Way...No Regrets\"\\nAlways Make Happ...  2010-03-25 21:16:20   \n",
      "13  Official account of the Africa Youth Advisory ...  2019-05-13 06:27:47   \n",
      "14                   Breaking news alerts from India.  2016-10-22 09:18:42   \n",
      "15  strive to promote Truth with Integrity.\\nhttps...  2019-11-11 12:02:27   \n",
      "16  Individual tweeting about significant happenin...  2009-04-26 09:38:11   \n",
      "17  Progressive mind. Flemish. Into movies, politi...  2009-06-13 13:48:16   \n",
      "18                                                NaN  2011-01-30 18:40:16   \n",
      "19                   COVID-19 Practice of Lechner Law  2019-12-03 19:00:11   \n",
      "20                                                NaN  2011-05-28 15:28:21   \n",
      "21  Latest #Africa & #MENA related #News releases ...  2011-02-22 09:09:45   \n",
      "22  Associate Professor of Economics (PhD in Econo...  2011-07-22 13:41:42   \n",
      "23  Providing training, education and technical co...  2009-08-06 02:43:36   \n",
      "24  Niko i ni≈°ta, professor, so-called Balkan expe...  2009-06-18 09:46:10   \n",
      "25  Focused on matching blood donors with those in...  2008-12-23 07:55:39   \n",
      "26  A leader in #GlobalDev. We work to improve liv...  2010-08-06 19:09:22   \n",
      "27  These days, I expose colonizers & exploits @ t...  2008-02-23 19:02:29   \n",
      "28  Journalist/Stay Grounded/ RT's are not Endorse...  2011-06-12 18:18:43   \n",
      "29  https://t.co/VDCHungubm | @1984Vivika | https:...  2019-12-06 14:42:15   \n",
      "\n",
      "    user_followers  user_friends  user_favourites  user_verified  \\\n",
      "0              624           950            18775          False   \n",
      "1             2253          1677               24           True   \n",
      "2             9275          9525             7254          False   \n",
      "3              197           987             1488          False   \n",
      "4           101009           168              101          False   \n",
      "5             1180          1071             1287          False   \n",
      "6            79956         54810             3801          False   \n",
      "7              608           355               95          False   \n",
      "8               25            29               18          False   \n",
      "9            55201         34239            29802          False   \n",
      "10               8            10                7          False   \n",
      "11             241          1694             8443          False   \n",
      "12               0             8               32          False   \n",
      "13             830           254             3692          False   \n",
      "14             546            29               88          False   \n",
      "15             274            32              378          False   \n",
      "16            3987            53              749          False   \n",
      "17             283          1432             1546          False   \n",
      "18              46           108              453          False   \n",
      "19              14            24               74          False   \n",
      "20             292          1037               58          False   \n",
      "21           10661             6             2037           True   \n",
      "22             751           183             1308          False   \n",
      "23            3038          2713             7445          False   \n",
      "24           18145          1389            13578          False   \n",
      "25         1215920          2047            19359           True   \n",
      "26           20699           458             2680          False   \n",
      "27              86           259             9412          False   \n",
      "28            8205          1083            32076          False   \n",
      "29              28          1449             1196          False   \n",
      "\n",
      "                   date                                               text  \\\n",
      "0   2020-07-25 12:27:21  If I smelled the scent of hand sanitizers toda...   \n",
      "1   2020-07-25 12:27:17  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
      "2   2020-07-25 12:27:14  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
      "3   2020-07-25 12:27:10  @brookbanktv The one gift #COVID19 has give me...   \n",
      "4   2020-07-25 12:27:08  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
      "5   2020-07-25 12:27:06  #coronavirus #covid19 deaths continue to rise....   \n",
      "6   2020-07-25 12:27:03  How #COVID19 Will Change Work in General (and ...   \n",
      "7   2020-07-25 12:27:00  You now have to wear face coverings when out s...   \n",
      "8   2020-07-25 12:26:59  Praying for good health and recovery of @Chouh...   \n",
      "9   2020-07-25 12:26:54  POPE AS GOD - Prophet Sadhu Sundar Selvaraj. W...   \n",
      "10  2020-07-25 12:26:53  49K+ Covid19 cases still no response from \\n@c...   \n",
      "11  2020-07-25 12:26:50  Order here: https://t.co/4NUrGX6EmA\\n\\n#logo #...   \n",
      "12  2020-07-25 12:26:47  üëãüèª@PattyHajdu @NavdeepSBains ‚Äî no one will be ...   \n",
      "13  2020-07-25 12:26:47  Let's all protect ourselves from #COVID19.\\nIt...   \n",
      "14  2020-07-25 12:26:46  Rajasthan Government today started a Plasma Ba...   \n",
      "15  2020-07-25 12:26:45  Nagaland police on Covid-19 Awareness at City ...   \n",
      "16  2020-07-25 12:26:44  July 25 #COVID19 update\\n#TamilNadu - 6988\\nDi...   \n",
      "17  2020-07-25 12:26:44  Second wave of #COVID19 in Flanders..back to m...   \n",
      "18  2020-07-25 12:26:43  It is during our darkest moments that we must ...   \n",
      "19  2020-07-25 12:26:39  COVID Update: The infection rate in Florida is...   \n",
      "20  2020-07-25 12:26:37  @EvanAKilgore @realDonaldTrump Good Patriots!\\...   \n",
      "21  2020-07-25 12:26:31  Coronavirus - South Africa: COVID-19 update fo...   \n",
      "22  2020-07-25 12:26:31  @JimBnntt Your image doesn't list a source, bu...   \n",
      "23  2020-07-25 12:26:30  The first comprehensive review of #WASH &amp; ...   \n",
      "24  2020-07-25 12:26:28  Holy water in times of #COVID19 https://t.co/Y...   \n",
      "25  2020-07-25 12:26:26  #Kolar\\nNeed #Blood Type :  B-positive\\nAt : J...   \n",
      "26  2020-07-25 12:26:26  Our Munitions Response Team in #BosniaandHerze...   \n",
      "27  2020-07-25 12:26:26  I can imagine the same people profiting off th...   \n",
      "28  2020-07-25 12:26:24  #TNCoronaUpdate\\n\\n#TN crosses 2 lakh mark and...   \n",
      "29  2020-07-25 12:26:21  @ratasjuri TAX Reform\\n\\nTax-free minimum:\\nFo...   \n",
      "\n",
      "                                             hashtags               source  \\\n",
      "0                                                 NaN   Twitter for iPhone   \n",
      "1                                                 NaN  Twitter for Android   \n",
      "2                                         ['COVID19']  Twitter for Android   \n",
      "3                                         ['COVID19']   Twitter for iPhone   \n",
      "4                   ['CoronaVirusUpdates', 'COVID19']  Twitter for Android   \n",
      "5                          ['coronavirus', 'covid19']      Twitter Web App   \n",
      "6                           ['COVID19', 'Recruiting']               Buffer   \n",
      "7                                                 NaN            TweetDeck   \n",
      "8                        ['covid19', 'covidPositive']  Twitter for Android   \n",
      "9                       ['HurricaneHanna', 'COVID19']   Twitter for iPhone   \n",
      "10                                                NaN      Twitter Web App   \n",
      "11  ['logo', 'graphicdesigner', 'logodesign', 'log...      Twitter Web App   \n",
      "12                                        ['COVID19']      Twitter Web App   \n",
      "13                                        ['COVID19']      Twitter Web App   \n",
      "14                                                NaN      Twitter Web App   \n",
      "15                  ['Covid19', 'keepsocialdistance']  Twitter for Android   \n",
      "16                ['COVID19', 'TamilNadu', 'chennai']   Twitter for iPhone   \n",
      "17                            ['COVID19', 'homework']  Twitter for Android   \n",
      "18                                          ['light']      Twitter Web App   \n",
      "19                                                NaN     Twitter for iPad   \n",
      "20                                                NaN   Twitter for iPhone   \n",
      "21                                                NaN      Africa Newsroom   \n",
      "22                                                NaN      Twitter Web App   \n",
      "23                                ['WASH', 'COVID19']   Twitter for iPhone   \n",
      "24                                        ['COVID19']  Twitter for Android   \n",
      "25                      ['Kolar', 'Blood', 'COVID19']   Blood Donors India   \n",
      "26                           ['BosniaandHerzegovina']      Twitter Web App   \n",
      "27                                        ['COVID19']  Twitter for Android   \n",
      "28                ['TNCoronaUpdate', 'TN', 'COVID19']  Twitter for Android   \n",
      "29                                                NaN     Twitter for iPad   \n",
      "\n",
      "    is_retweet  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n",
      "5        False  \n",
      "6        False  \n",
      "7        False  \n",
      "8        False  \n",
      "9        False  \n",
      "10       False  \n",
      "11       False  \n",
      "12       False  \n",
      "13       False  \n",
      "14       False  \n",
      "15       False  \n",
      "16       False  \n",
      "17       False  \n",
      "18       False  \n",
      "19       False  \n",
      "20       False  \n",
      "21       False  \n",
      "22       False  \n",
      "23       False  \n",
      "24       False  \n",
      "25       False  \n",
      "26       False  \n",
      "27       False  \n",
      "28       False  \n",
      "29       False  \n"
     ]
    }
   ],
   "source": [
    "## If you comment out the commands below, you can see the format of the dataframe. \n",
    "## The first column is an index, then you have user_name, user_location, user_description,\n",
    "## user_created, user_followers, user_friends, user_favourites, user_verified (True/False),\n",
    "## date, text, hashtags (presented as a list of strings), \n",
    "## source (Twitter Web App, for android, etc), and is_retweet (True/False)\n",
    "\n",
    "##print(len(tweets))\n",
    "##print(tweets.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166656"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Our first goal is to extract a subset of the tweet texts, labeled according to political\n",
    "## leaning. It's hard to extract someone's political stance just from the information\n",
    "## available in this table, but we will make a guess that a user_description that\n",
    "## uses a lot of right-wing adjectives (conservative, traditional, etc.) is probably\n",
    "## conservative and a user_description that uses a lot of left-wing adjectives (liberal,\n",
    "## progressive, etc.) is probably liberal. Of course, many people leave their descriptions blank\n",
    "## or don't include such terms; we will simply omit their posts. \n",
    "\n",
    "## The first thing we do is define a function that will extract the political leaning\n",
    "## of a tweet's author. We build it out of somewhat simpler functions: \n",
    "## first a function that checks if a word is in a user_description, then a function\n",
    "## that checks if any of a list of words appears in a user_description,\n",
    "## and finally a function that checks to see if \n",
    "\n",
    "def word_in_desc(word,desc):\n",
    "    return word in str(desc)\n",
    "\n",
    "def some_word_in_desc(lst,desc):\n",
    "    state = False\n",
    "    for word in lst:\n",
    "        if word_in_desc(word,desc):\n",
    "            state = True\n",
    "            break\n",
    "    return state\n",
    "\n",
    "def political_leaning(desc, \n",
    "                      liberal_words=['liberal','progressive','democratic','leftist'],\n",
    "                      conservative_words=['conservative','traditional','libertarian','republican']):\n",
    "    if some_word_in_desc(liberal_words,desc) and not some_word_in_desc(conservative_words,desc):\n",
    "        return \"L\"\n",
    "    elif some_word_in_desc(conservative_words,desc) and not some_word_in_desc(liberal_words,desc):\n",
    "        return \"R\"\n",
    "    else:\n",
    "        return \"I\"\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We should check that this works at least somewhat reasonably. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next function that we want to create will take a tweet and determine its sentiment\n",
    "## that is, whether it is positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lst = []\n",
    "for i in range(len(tweets)):\n",
    "    tweet_lst.append(tweets.loc[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voices from the Belt and Road: COVID-19 rap song alerts to needed precautions when returning to work\n",
      "#beltandroad‚Ä¶ https://t.co/O39NmfUDmc\n"
     ]
    }
   ],
   "source": [
    "##Now we use a less naive approach to analyze the sentiment. \n",
    "\n",
    "print(tweets.loc[100]['text'])\n",
    "\n",
    "link_indices = []\n",
    "no_link_indices = []\n",
    "\n",
    "for i in range(len(tweet_lst)): \n",
    "    if 'https' in tweet_lst[i]:\n",
    "        link_indices.append(i)\n",
    "    else:\n",
    "        no_link_indices.append(i)\n",
    "\n",
    "# print(len(link_indices))\n",
    "# print(len(no_link_indices))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in no_link_indices:\n",
    "#     print(tweet_lst[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "\n",
    "## First we are going to use some standard training tweets to create a model. \n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "\n",
    "tknz = TweetTokenizer()\n",
    "\n",
    "tweet_text_tokenizes = [tknz.tokenize(tweet) for tweet in tweet_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['49K', '+', 'Covid', '19', 'cases', 'still', 'no', 'response', 'from', '@cbseindia29', '@HRDMinistry', '@DrRPNishank', '.', 'Please', 'cancel', 'the', 'compartment', 'exa', '‚Ä¶', 'https://t.co/kV2ZKmumu1']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_text_tokenizes[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "## First we use part-of-speech tagging. \n",
    "## We can remove the \n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "print(pos_tag(tweet_tokens[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tweet_tokens[140])\n",
    "# print(pos_tag(tweet_tokens[140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to process the words. The first step is Lemmatization: reducing a word to its root or canonical form.\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "## If you are missing the two modules, uncomment the code. \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# The following function uses the pos_tag information to feed the lemmatizer (which can only operate when it \n",
    "## knows what part of speech the input word is)\n",
    "# def lemmatize_sentence(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_sentence = []\n",
    "#     for word, tag in pos_tag(tokens):\n",
    "#         if tag.startswith('NN'):\n",
    "#             pos = 'n'\n",
    "#         elif tag.startswith('VB'):\n",
    "#             pos = 'v'\n",
    "#         else:\n",
    "#             pos = 'a'\n",
    "#         lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "#     return lemmatized_sentence\n",
    "\n",
    "# print(lemmatize_sentence(tweet_tokens[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-c0703554a795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcleaned_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_token_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet_token_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_text_tokenizes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-c0703554a795>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcleaned_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_token_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet_token_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_text_tokenizes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-c0703554a795>\u001b[0m in \u001b[0;36mremove_noise\u001b[0;34m(tweet_tokens, stop_words)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcleaned_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n\u001b[1;32m     18\u001b[0m                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m    160\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## NOW we need to clean up the lemmatized posts. \n",
    "\n",
    "## We do this via a function called remove_noise that begins by removing URLS (identified as a regular expression),\n",
    "## and twitter handles (identified with a @ symbol). \n",
    "\n",
    "## We incorporate the lemmatization from the previous thing into this function. Note that we also have a \n",
    "## thing that filters out punctuation, as well as the so-called stop words,\n",
    "##  which are in the typle of stop_words provided as the second argument)\n",
    "\n",
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "covid_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in tweet_text_tokenizes:\n",
    "    covid_cleaned_tokens_list.append(remove_noise(tokens,stop_words))\n",
    "    \n",
    "## Here is an example of what remove noise does: the first list is the list of tokens in the 500th\n",
    "## positive tweet, the second is what happens to that list after we delete all the 'junk'\n",
    "\n",
    "\n",
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next big step is to actually measure how frequently each word turns up in the set of tweets. \n",
    "## This is called measuring the word density. \n",
    "\n",
    "# def get_all_words(list_of_list_of_words):\n",
    "#     for lst in list_of_list_of_words:\n",
    "#         for word in lst:\n",
    "#             yield word\n",
    "            \n",
    "# ## how this works: lists are iterables, which basically means that for-loops can range over them\n",
    "# ## a generator is a special kind of iterable which you use to create a list, and you can only iterate through once\n",
    "# ## this creates a list. (we could also have written it differently: \n",
    "# ## return [x for all x in lst for all lst in list_of_list_of_words]\n",
    "\n",
    "# all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "# all_neg_words = get_all_words(negative_cleaned_tokens_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import FreqDist\n",
    "\n",
    "# freq_dist_pos = FreqDist(all_pos_words)\n",
    "# freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "# # print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are going to use Na√Øve Bayes for our classification. The NB classifier in NLTK requires the \n",
    "## tweets to be converted to dictionary format, with the words as keys and True for each value.\n",
    "\n",
    "\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets label the training tweets as either Positive or Negative\n",
    "import random\n",
    "## we import the random module because we will randomly shuffle the dataset, \n",
    "## after it's been labeled, into a training chunk and a testing chunk\n",
    "\n",
    "positive_dataset = [(tweet_dict,\"Positive\")\n",
    "                   for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict,\"Negative\")\n",
    "                   for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9963333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2074.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1647.2 : 1.0\n",
      "                follower = True           Positi : Negati =     38.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     26.2 : 1.0\n",
      "                 welcome = True           Positi : Negati =     21.4 : 1.0\n",
      "                     bam = True           Positi : Negati =     21.0 : 1.0\n",
      "                    glad = True           Positi : Negati =     20.3 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.4 : 1.0\n",
      "                    blog = True           Positi : Negati =     15.0 : 1.0\n",
      "               community = True           Positi : Negati =     14.3 : 1.0\n",
      "                   didnt = True           Negati : Positi =     13.7 : 1.0\n",
      "                  arrive = True           Positi : Negati =     13.5 : 1.0\n",
      "                     idk = True           Negati : Positi =     12.4 : 1.0\n",
      "           unfortunately = True           Negati : Positi =     12.4 : 1.0\n",
      "                follback = True           Positi : Negati =     12.3 : 1.0\n",
      "                     via = True           Positi : Negati =     11.3 : 1.0\n",
      "                     ugh = True           Negati : Positi =     11.0 : 1.0\n",
      "                      aw = True           Negati : Positi =     10.6 : 1.0\n",
      "                  excite = True           Positi : Negati =     10.3 : 1.0\n",
      "               goodnight = True           Positi : Negati =     10.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Now we get to some of the actual action -- building and training the model. \n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def string_to_classification(string):\n",
    "    tokens = remove_noise(word_tokenize(string))\n",
    "    print('The classification is: ',classifier.classify(dict([token,True] for token in tokens)))\n",
    "    probs = classifier.prob_classify(dict([token,True] for token in tokens))\n",
    "    print('Negative probability ',probs.prob('Negative'))\n",
    "    print('Positive probability ',probs.prob('Positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification is:  Negative\n",
      "Negative probability  0.8439478115392183\n",
      "Positive probability  0.15605218846078261\n"
     ]
    }
   ],
   "source": [
    "string_to_classification('people aren‚Äôt aware of this but HW assassinated 25 percent of Americans after the convention and that explains this shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now lets start actually processing the Covid tweets. \n",
    "\n",
    "## Our question is: if we can tag a Covid-related tweet as coming from a \n",
    "## politically conservative or politically liberal account, is that \n",
    "## associated with the sentiment as predicted by our Naive Bayes classifier? \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
